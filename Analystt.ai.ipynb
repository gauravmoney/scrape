{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906b5ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product listings scraped and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_product_listings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_listings = []\n",
    "    \n",
    "    for product in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
    "        product_url = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal'})['href']\n",
    "        product_name = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "        product_price = product.find('span', {'class': 'a-offscreen'}).text.strip()\n",
    "        product_rating = product.find('span', {'class': 'a-icon-alt'}).text.strip().split()[0]\n",
    "        num_reviews = product.find('span', {'class': 'a-size-base'}).text.strip()\n",
    "        \n",
    "        product_listings.append({\n",
    "            'Product URL': product_url,\n",
    "            'Product Name': product_name,\n",
    "            'Product Price': product_price,\n",
    "            'Rating': product_rating,\n",
    "            'Number of Reviews': num_reviews,\n",
    "        })\n",
    "    \n",
    "    return product_listings\n",
    "\n",
    "def scrape_multiple_pages(base_url, num_pages):\n",
    "    all_product_listings = []\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        url = f\"{base_url}&page={page_num}\"\n",
    "        product_listings = scrape_product_listings(url)\n",
    "        all_product_listings.extend(product_listings)\n",
    "        time.sleep(2)  \n",
    "        \n",
    "    return all_product_listings\n",
    "\n",
    "\n",
    "base_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
    "num_pages_to_scrape = 20\n",
    "\n",
    "all_product_listings = scrape_multiple_pages(base_url, num_pages_to_scrape)\n",
    "\n",
    "csv_filename = 'amazon_bags_data.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_product_listings)\n",
    "\n",
    "print(\"Product listings scraped and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35348b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\AppData\\Local\\Temp\\ipykernel_10956\\770284498.py:10: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  asin = soup.find('th', text='ASIN')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional product information scraped and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    asin = soup.find('th', text='ASIN')\n",
    "    if asin:\n",
    "        asin = asin.find_next_sibling('td').text.strip()\n",
    "    else:\n",
    "        asin = 'N/A'\n",
    "    \n",
    "    description_meta = soup.find('meta', {'name': 'description'})\n",
    "    if description_meta:\n",
    "        description = description_meta['content'].strip()\n",
    "    else:\n",
    "        description = 'N/A'\n",
    "    \n",
    "    manufacturer_tag = soup.find('a', {'id': 'bylineInfo'})\n",
    "    if manufacturer_tag:\n",
    "        manufacturer = manufacturer_tag.text.strip()\n",
    "    else:\n",
    "        manufacturer = 'N/A'\n",
    "    \n",
    "    product_description_tag = soup.find('div', {'id': 'productDescription'})\n",
    "    if product_description_tag:\n",
    "        product_description = product_description_tag.text.strip()\n",
    "    else:\n",
    "        product_description = 'N/A'\n",
    "    \n",
    "    return {\n",
    "        'Product URL': url,\n",
    "        'ASIN': asin,\n",
    "        'Description': description,\n",
    "        'Manufacturer': manufacturer,\n",
    "        'Product Description': product_description,\n",
    "    }\n",
    "\n",
    "def scrape_multiple_products(urls):\n",
    "    products_info = []\n",
    "    for url in urls:\n",
    "        product_info = scrape_product_details(url)\n",
    "        products_info.append(product_info)\n",
    "        time.sleep(2)  # Be respectful by adding a delay between requests\n",
    "            \n",
    "    return products_info\n",
    "\n",
    "\n",
    "\n",
    "product_urls = []\n",
    "with open('amazon_bags_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        product_urls.append(row['Product URL'])\n",
    "\n",
    "product_details = scrape_multiple_products(product_urls)\n",
    "\n",
    "csv_filename_part2 = 'amazon_bags_additional_info.csv'\n",
    "with open(csv_filename_part2, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Product URL', 'ASIN', 'Description', 'Manufacturer', 'Product Description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_details)\n",
    "\n",
    "print(\"Additional product information scraped and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590d564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
